DEBUG: call_ollama_api started with prompt length: 32, system_prompt length: 27
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 50
DEBUG: Full prompt length: 63
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 551 characters
DEBUG: Successfully extracted response, length: 36
DEBUG: call_ollama_api started with prompt length: 3336, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 8192
DEBUG: Full prompt length: 3509
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 12558 characters
DEBUG: Successfully extracted response, length: 4646
DEBUG: call_ollama_api started with prompt length: 5816, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 5836
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16511 characters
DEBUG: Successfully extracted response, length: 5360
DEBUG: call_ollama_api started with prompt length: 6460, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6479
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16635 characters
DEBUG: Successfully extracted response, length: 5204
DEBUG: call_ollama_api started with prompt length: 6374, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6394
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 21337 characters
DEBUG: Successfully extracted response, length: 7661
DEBUG: call_ollama_api started with prompt length: 7018, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7037
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 17279 characters
DEBUG: Successfully extracted response, length: 5244
DEBUG: call_ollama_api started with prompt length: 6414, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6434
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 9917 characters
DEBUG: Successfully extracted response, length: 1721
DEBUG: call_ollama_api started with prompt length: 7058, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7077
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15709 characters
DEBUG: Successfully extracted response, length: 4436
DEBUG: call_ollama_api started with prompt length: 5606, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 5626
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 10212 characters
DEBUG: Successfully extracted response, length: 2280
DEBUG: call_ollama_api started with prompt length: 6250, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6269
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16107 characters
DEBUG: Successfully extracted response, length: 4988
DEBUG: call_ollama_api started with prompt length: 6158, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6178
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 18883 characters
DEBUG: Successfully extracted response, length: 6400
DEBUG: call_ollama_api started with prompt length: 6802, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6821
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 17739 characters
DEBUG: Successfully extracted response, length: 5535
DEBUG: call_ollama_api started with prompt length: 6705, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6725
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 10842 characters
DEBUG: Successfully extracted response, length: 2069
DEBUG: call_ollama_api started with prompt length: 8910, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 8192
DEBUG: Full prompt length: 9083
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 19070 characters
DEBUG: Successfully extracted response, length: 4933
DEBUG: call_ollama_api started with prompt length: 6103, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6123
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 10645 characters
DEBUG: Successfully extracted response, length: 1987
DEBUG: call_ollama_api started with prompt length: 6747, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6766
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 4693, system_prompt length: 169
DEBUG: Using model: mistral:latest, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 4866
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 1216, system_prompt length: 725
DEBUG: Using model: mistral:latest, temperature: 0.7, max_tokens: 50000
DEBUG: Full prompt length: 1945
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 10626 characters
DEBUG: Successfully extracted response, length: 3169
DEBUG: call_ollama_api started with prompt length: 3513, system_prompt length: 725
DEBUG: Using model: mistral:latest, temperature: 0.7, max_tokens: 50000
DEBUG: Full prompt length: 4242
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15087 characters
DEBUG: Successfully extracted response, length: 3847
DEBUG: call_ollama_api started with prompt length: 4209, system_prompt length: 725
DEBUG: Using model: mistral:latest, temperature: 0.7, max_tokens: 50000
DEBUG: Full prompt length: 4938
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15981 characters
DEBUG: Successfully extracted response, length: 3895
DEBUG: call_ollama_api started with prompt length: 915, system_prompt length: 725
DEBUG: Using model: mistral:latest, temperature: 0.7, max_tokens: 50000
DEBUG: Full prompt length: 1644
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 12847 characters
DEBUG: Successfully extracted response, length: 4261
DEBUG: call_ollama_api started with prompt length: 4605, system_prompt length: 725
DEBUG: Using model: mistral:latest, temperature: 0.7, max_tokens: 50000
DEBUG: Full prompt length: 5334
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 14667 characters
DEBUG: Successfully extracted response, length: 3140
DEBUG: call_ollama_api started with prompt length: 3502, system_prompt length: 725
DEBUG: Using model: mistral:latest, temperature: 0.7, max_tokens: 50000
DEBUG: Full prompt length: 4231
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15328 characters
DEBUG: Successfully extracted response, length: 4168
DEBUG: call_ollama_api started with prompt length: 4344, system_prompt length: 169
DEBUG: Using model: mistral:latest, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 4517
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15975 characters
DEBUG: Successfully extracted response, length: 4754
DEBUG: call_ollama_api started with prompt length: 5924, system_prompt length: 16
DEBUG: Using model: mistral:latest, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 5944
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 9280 characters
DEBUG: Successfully extracted response, length: 789
DEBUG: call_ollama_api started with prompt length: 6568, system_prompt length: 15
DEBUG: Using model: mistral:latest, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6587
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 20562 characters
DEBUG: Successfully extracted response, length: 5574
DEBUG: call_ollama_api started with prompt length: 6744, system_prompt length: 16
DEBUG: Using model: mistral:latest, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6764
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 10611 characters
DEBUG: Successfully extracted response, length: 707
DEBUG: call_ollama_api started with prompt length: 7388, system_prompt length: 15
DEBUG: Using model: mistral:latest, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7407
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 0 characters
DEBUG: Failed to extract .response from JSON, raw response: 
DEBUG: call_ollama_api started with prompt length: 6744, system_prompt length: 16
DEBUG: Using model: mistral:latest, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6764
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 10785 characters
DEBUG: Successfully extracted response, length: 763
DEBUG: call_ollama_api started with prompt length: 7388, system_prompt length: 15
DEBUG: Using model: mistral:latest, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7407
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 9941, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 10114
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 19435 characters
DEBUG: Successfully extracted response, length: 5199
DEBUG: call_ollama_api started with prompt length: 9566, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 9739
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 0 characters
DEBUG: Failed to extract .response from JSON, raw response: 
DEBUG: call_ollama_api started with prompt length: 9566, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 9739
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 28298 characters
DEBUG: Successfully extracted response, length: 10137
DEBUG: call_ollama_api started with prompt length: 11307, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 11327
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 26641 characters
DEBUG: Successfully extracted response, length: 8394
DEBUG: call_ollama_api started with prompt length: 11951, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 11970
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 23958 characters
DEBUG: Successfully extracted response, length: 6686
DEBUG: call_ollama_api started with prompt length: 7856, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7876
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 12191 characters
DEBUG: Successfully extracted response, length: 2346
DEBUG: call_ollama_api started with prompt length: 8500, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 8519
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 24453 characters
DEBUG: Successfully extracted response, length: 8359
DEBUG: call_ollama_api started with prompt length: 9529, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 9549
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 21873 characters
DEBUG: Successfully extracted response, length: 6466
DEBUG: call_ollama_api started with prompt length: 10173, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 10192
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 25585 characters
DEBUG: Successfully extracted response, length: 8087
DEBUG: call_ollama_api started with prompt length: 9257, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 9277
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 0 characters
DEBUG: Failed to extract .response from JSON, raw response: 
DEBUG: call_ollama_api started with prompt length: 9901, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 9920
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 25227 characters
DEBUG: Successfully extracted response, length: 8078
DEBUG: call_ollama_api started with prompt length: 9248, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 9268
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 20571 characters
DEBUG: Successfully extracted response, length: 6061
DEBUG: call_ollama_api started with prompt length: 9892, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 9911
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 24700 characters
DEBUG: Successfully extracted response, length: 7801
DEBUG: call_ollama_api started with prompt length: 8971, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 8991
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 22161 characters
DEBUG: Successfully extracted response, length: 6984
DEBUG: call_ollama_api started with prompt length: 12174, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 12347
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 26747 characters
DEBUG: Successfully extracted response, length: 7799
DEBUG: call_ollama_api started with prompt length: 8969, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 8989
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 23424 characters
DEBUG: Successfully extracted response, length: 7538
DEBUG: call_ollama_api started with prompt length: 9613, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 9632
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 24483 characters
DEBUG: Successfully extracted response, length: 7799
DEBUG: call_ollama_api started with prompt length: 8969, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 8989
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 22168 characters
DEBUG: Successfully extracted response, length: 6938
DEBUG: call_ollama_api started with prompt length: 9613, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 9632
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 24529 characters
DEBUG: Successfully extracted response, length: 7815
DEBUG: call_ollama_api started with prompt length: 8985, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 9005
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 20076 characters
DEBUG: Successfully extracted response, length: 5852
DEBUG: call_ollama_api started with prompt length: 9629, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 9648
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 24560 characters
DEBUG: Successfully extracted response, length: 7815
DEBUG: call_ollama_api started with prompt length: 8985, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 9005
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 22830 characters
DEBUG: Successfully extracted response, length: 7242
DEBUG: call_ollama_api started with prompt length: 9629, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 9648
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 24256 characters
DEBUG: Successfully extracted response, length: 7650
DEBUG: call_ollama_api started with prompt length: 8820, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 8840
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 21907 characters
DEBUG: Successfully extracted response, length: 6855
DEBUG: call_ollama_api started with prompt length: 9464, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 9483
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 24077 characters
DEBUG: Successfully extracted response, length: 7628
DEBUG: call_ollama_api started with prompt length: 8798, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 8818
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 23488 characters
DEBUG: Successfully extracted response, length: 7633
DEBUG: call_ollama_api started with prompt length: 19827, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 20000
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 28104 characters
DEBUG: Successfully extracted response, length: 4895
DEBUG: call_ollama_api started with prompt length: 6065, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6085
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 9175 characters
DEBUG: Successfully extracted response, length: 1568
DEBUG: call_ollama_api started with prompt length: 6709, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6728
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 19705 characters
DEBUG: Successfully extracted response, length: 6775
DEBUG: call_ollama_api started with prompt length: 7945, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7965
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 13612 characters
DEBUG: Successfully extracted response, length: 2967
DEBUG: call_ollama_api started with prompt length: 8589, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 8608
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 20347 characters
DEBUG: Successfully extracted response, length: 6069
DEBUG: call_ollama_api started with prompt length: 7239, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7259
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 7727 characters
DEBUG: Successfully extracted response, length: 126
DEBUG: call_ollama_api started with prompt length: 7883, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7902
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 23208 characters
DEBUG: Successfully extracted response, length: 7797
DEBUG: call_ollama_api started with prompt length: 8967, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 8987
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 14115 characters
DEBUG: Successfully extracted response, length: 2567
DEBUG: call_ollama_api started with prompt length: 9611, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 9630
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 31724 characters
DEBUG: Successfully extracted response, length: 11579
DEBUG: call_ollama_api started with prompt length: 12749, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 12769
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16924 characters
DEBUG: Successfully extracted response, length: 2515
DEBUG: call_ollama_api started with prompt length: 13063, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 13082
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 22442 characters
DEBUG: Successfully extracted response, length: 5174
DEBUG: call_ollama_api started with prompt length: 6344, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6364
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 10015 characters
DEBUG: Successfully extracted response, length: 1825
DEBUG: call_ollama_api started with prompt length: 25017, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 25190
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 35319 characters
DEBUG: Successfully extracted response, length: 6112
DEBUG: call_ollama_api started with prompt length: 7282, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7302
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 12344 characters
DEBUG: Successfully extracted response, length: 2650
DEBUG: call_ollama_api started with prompt length: 7926, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7945
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 12168, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 12341
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 25403 characters
DEBUG: Successfully extracted response, length: 7118
DEBUG: call_ollama_api started with prompt length: 8288, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 8308
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 11485, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 11658
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 26870 characters
DEBUG: Successfully extracted response, length: 8220
DEBUG: call_ollama_api started with prompt length: 9390, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 9410
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 9246 characters
DEBUG: Successfully extracted response, length: 23
DEBUG: call_ollama_api started with prompt length: 9704, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 9723
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 12587, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 12760
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 22917 characters
DEBUG: Successfully extracted response, length: 5711
DEBUG: call_ollama_api started with prompt length: 6881, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6901
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 10078, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 10251
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 20945 characters
DEBUG: Successfully extracted response, length: 5994
DEBUG: call_ollama_api started with prompt length: 7164, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7184
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16587 characters
DEBUG: Successfully extracted response, length: 5113
DEBUG: call_ollama_api started with prompt length: 7808, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7827
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 10361, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 10534
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 17367 characters
DEBUG: Successfully extracted response, length: 4092
DEBUG: call_ollama_api started with prompt length: 10361, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 10534
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 21056 characters
DEBUG: Successfully extracted response, length: 5994
DEBUG: call_ollama_api started with prompt length: 7164, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7184
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 7014 characters
DEBUG: Successfully extracted response, length: 33
DEBUG: call_ollama_api started with prompt length: 7808, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7827
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 22138 characters
DEBUG: Successfully extracted response, length: 7603
DEBUG: call_ollama_api started with prompt length: 8773, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 8793
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 23501 characters
DEBUG: Successfully extracted response, length: 7735
DEBUG: call_ollama_api started with prompt length: 9087, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 9106
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 11970, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 12143
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 28177 characters
DEBUG: Successfully extracted response, length: 8705
DEBUG: call_ollama_api started with prompt length: 9875, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 9895
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 13072, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 13245
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 30201 characters
DEBUG: Successfully extracted response, length: 9161
DEBUG: call_ollama_api started with prompt length: 10331, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 10351
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15542 characters
DEBUG: Successfully extracted response, length: 2769
DEBUG: call_ollama_api started with prompt length: 10645, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 10664
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 27070 characters
DEBUG: Successfully extracted response, length: 8585
DEBUG: call_ollama_api started with prompt length: 9755, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 9775
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 14340 characters
DEBUG: Successfully extracted response, length: 2460
DEBUG: call_ollama_api started with prompt length: 10069, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 10088
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 27360 characters
DEBUG: Successfully extracted response, length: 9166
DEBUG: call_ollama_api started with prompt length: 10336, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 10356
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 14318 characters
DEBUG: Successfully extracted response, length: 2279
DEBUG: call_ollama_api started with prompt length: 10650, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 10669
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 27376 characters
DEBUG: Successfully extracted response, length: 9043
DEBUG: call_ollama_api started with prompt length: 10213, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 10233
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 14921 characters
DEBUG: Successfully extracted response, length: 2696
DEBUG: call_ollama_api started with prompt length: 10527, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 10546
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 24045 characters
DEBUG: Successfully extracted response, length: 7247
DEBUG: call_ollama_api started with prompt length: 8417, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 8437
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 13019 characters
DEBUG: Successfully extracted response, length: 2394
DEBUG: call_ollama_api started with prompt length: 8731, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 8750
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 28346 characters
DEBUG: Successfully extracted response, length: 10190
DEBUG: call_ollama_api started with prompt length: 11360, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 11380
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16188 characters
DEBUG: Successfully extracted response, length: 2636
DEBUG: call_ollama_api started with prompt length: 14563, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 14736
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 21273 characters
DEBUG: Successfully extracted response, length: 3693
DEBUG: call_ollama_api started with prompt length: 4863, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 4883
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15635 characters
DEBUG: Successfully extracted response, length: 5216
DEBUG: call_ollama_api started with prompt length: 5507, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 5526
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 12027 characters
DEBUG: Successfully extracted response, length: 3121
DEBUG: call_ollama_api started with prompt length: 4291, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 4311
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15804 characters
DEBUG: Successfully extracted response, length: 5734
DEBUG: call_ollama_api started with prompt length: 4935, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 4954
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 24527 characters
DEBUG: Successfully extracted response, length: 9749
DEBUG: call_ollama_api started with prompt length: 10919, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 10939
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 14713 characters
DEBUG: Successfully extracted response, length: 1879
DEBUG: call_ollama_api started with prompt length: 11233, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 11252
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 34198 characters
DEBUG: Successfully extracted response, length: 11592
DEBUG: call_ollama_api started with prompt length: 12762, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 12782
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16115 characters
DEBUG: Successfully extracted response, length: 1678
DEBUG: call_ollama_api started with prompt length: 13076, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 13095
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 20128 characters
DEBUG: Successfully extracted response, length: 3570
DEBUG: call_ollama_api started with prompt length: 4740, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 4760
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 9858 characters
DEBUG: Successfully extracted response, length: 2379
DEBUG: call_ollama_api started with prompt length: 5384, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 5403
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 12788 characters
DEBUG: Successfully extracted response, length: 3578
DEBUG: call_ollama_api started with prompt length: 4748, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 4768
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 8712 characters
DEBUG: Successfully extracted response, length: 1884
DEBUG: call_ollama_api started with prompt length: 18166, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 18339
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 24557 characters
DEBUG: Successfully extracted response, length: 3660
DEBUG: call_ollama_api started with prompt length: 4830, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 4850
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 9311 characters
DEBUG: Successfully extracted response, length: 2073
DEBUG: call_ollama_api started with prompt length: 5474, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 5493
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 14864 characters
DEBUG: Successfully extracted response, length: 4642
DEBUG: call_ollama_api started with prompt length: 5812, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 5832
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 11158 characters
DEBUG: Successfully extracted response, length: 2609
DEBUG: call_ollama_api started with prompt length: 6456, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6475
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 19008 characters
DEBUG: Successfully extracted response, length: 6191
DEBUG: call_ollama_api started with prompt length: 7361, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7381
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 19979 characters
DEBUG: Successfully extracted response, length: 6224
DEBUG: call_ollama_api started with prompt length: 8005, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 8024
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 17760 characters
DEBUG: Successfully extracted response, length: 4808
DEBUG: call_ollama_api started with prompt length: 5978, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 5998
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15754 characters
DEBUG: Successfully extracted response, length: 4851
DEBUG: call_ollama_api started with prompt length: 6622, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6641
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16353 characters
DEBUG: Successfully extracted response, length: 4808
DEBUG: call_ollama_api started with prompt length: 5978, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 5998
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 21319 characters
DEBUG: Successfully extracted response, length: 7684
DEBUG: call_ollama_api started with prompt length: 6622, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6641
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 18736 characters
DEBUG: Successfully extracted response, length: 5992
DEBUG: call_ollama_api started with prompt length: 7162, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7182
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 22176 characters
DEBUG: Successfully extracted response, length: 7507
DEBUG: call_ollama_api started with prompt length: 24174, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 24347
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 31967 characters
DEBUG: Successfully extracted response, length: 4521
DEBUG: call_ollama_api started with prompt length: 5691, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 5711
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 9885 characters
DEBUG: Successfully extracted response, length: 2032
DEBUG: call_ollama_api started with prompt length: 6335, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6354
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 17508 characters
DEBUG: Successfully extracted response, length: 5660
DEBUG: call_ollama_api started with prompt length: 6830, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6850
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 18342 characters
DEBUG: Successfully extracted response, length: 5849
DEBUG: call_ollama_api started with prompt length: 7474, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7493
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16506 characters
DEBUG: Successfully extracted response, length: 4593
DEBUG: call_ollama_api started with prompt length: 5763, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 5783
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15517 characters
DEBUG: Successfully extracted response, length: 5012
DEBUG: call_ollama_api started with prompt length: 6407, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6426
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 0 characters
DEBUG: Failed to extract .response from JSON, raw response: 
DEBUG: call_ollama_api started with prompt length: 5763, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 5783
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 9597 characters
DEBUG: Successfully extracted response, length: 1914
DEBUG: call_ollama_api started with prompt length: 6407, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6426
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16851 characters
DEBUG: Successfully extracted response, length: 5239
DEBUG: call_ollama_api started with prompt length: 6409, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6429
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 10408 characters
DEBUG: Successfully extracted response, length: 1888
DEBUG: call_ollama_api started with prompt length: 7053, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7072
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 20771 characters
DEBUG: Successfully extracted response, length: 6759
DEBUG: call_ollama_api started with prompt length: 7929, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7949
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 8570 characters
DEBUG: Successfully extracted response, length: 123
DEBUG: call_ollama_api started with prompt length: 30960, system_prompt length: 169
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 31133
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 39059 characters
DEBUG: Successfully extracted response, length: 4617
DEBUG: call_ollama_api started with prompt length: 5787, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 5807
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 4693, system_prompt length: 169
DEBUG: Using model: gemma2:9b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 4866
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 14557, system_prompt length: 169
DEBUG: Using model: gemma2:9b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 14730
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 0 characters
DEBUG: Failed to extract .response from JSON, raw response: 
DEBUG: call_ollama_api started with prompt length: 14557, system_prompt length: 169
DEBUG: Using model: gemma2:9b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 14730
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 22979 characters
DEBUG: Successfully extracted response, length: 3485
DEBUG: call_ollama_api started with prompt length: 4655, system_prompt length: 16
DEBUG: Using model: gemma2:9b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 4675
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 7545 characters
DEBUG: Successfully extracted response, length: 1020
DEBUG: call_ollama_api started with prompt length: 5299, system_prompt length: 15
DEBUG: Using model: gemma2:9b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 5318
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 7852, system_prompt length: 169
DEBUG: Using model: gemma2:9b-instruct-q4_K_M, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 8025
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 14712 characters
DEBUG: Successfully extracted response, length: 3068
DEBUG: call_ollama_api started with prompt length: 4238, system_prompt length: 16
DEBUG: Using model: gemma2:9b-instruct-q4_K_M, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 4258
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 7852, system_prompt length: 169
DEBUG: Using model: gemma2:9b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 8025
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 14662 characters
DEBUG: Successfully extracted response, length: 3016
DEBUG: call_ollama_api started with prompt length: 4186, system_prompt length: 16
DEBUG: Using model: gemma2:9b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 4206
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 7383, system_prompt length: 169
DEBUG: Using model: gemma2:2b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 7556
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16945 characters
DEBUG: Successfully extracted response, length: 4400
DEBUG: call_ollama_api started with prompt length: 5570, system_prompt length: 16
DEBUG: Using model: gemma2:2b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 5590
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 8155 characters
DEBUG: Successfully extracted response, length: 837
DEBUG: call_ollama_api started with prompt length: 6214, system_prompt length: 15
DEBUG: Using model: gemma2:2b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6233
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 18613 characters
DEBUG: Successfully extracted response, length: 5720
DEBUG: call_ollama_api started with prompt length: 6890, system_prompt length: 16
DEBUG: Using model: gemma2:2b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6910
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 12128 characters
DEBUG: Successfully extracted response, length: 2080
DEBUG: call_ollama_api started with prompt length: 7534, system_prompt length: 15
DEBUG: Using model: gemma2:2b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7553
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 17985 characters
DEBUG: Successfully extracted response, length: 4597
DEBUG: call_ollama_api started with prompt length: 5767, system_prompt length: 16
DEBUG: Using model: gemma2:2b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 5787
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 11372 characters
DEBUG: Successfully extracted response, length: 2249
DEBUG: call_ollama_api started with prompt length: 6411, system_prompt length: 15
DEBUG: Using model: gemma2:2b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6430
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 17876 characters
DEBUG: Successfully extracted response, length: 5177
DEBUG: call_ollama_api started with prompt length: 6347, system_prompt length: 16
DEBUG: Using model: gemma2:2b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 6367
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 11895 characters
DEBUG: Successfully extracted response, length: 2339
DEBUG: call_ollama_api started with prompt length: 6991, system_prompt length: 15
DEBUG: Using model: gemma2:2b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 7010
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 9544, system_prompt length: 169
DEBUG: Using model: gemma2:2b, temperature: 0.8, max_tokens: 50000
DEBUG: Full prompt length: 9717
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 25073 characters
DEBUG: Successfully extracted response, length: 7187
DEBUG: call_ollama_api started with prompt length: 7633, system_prompt length: 177
DEBUG: Using model: gemma2:2b, temperature: 0.8, max_tokens: 30000
DEBUG: Full prompt length: 7814
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15330 characters
DEBUG: Successfully extracted response, length: 3358
DEBUG: call_ollama_api started with prompt length: 7633, system_prompt length: 177
DEBUG: Using model: gemma2:2b, temperature: 0.8, max_tokens: 30000
DEBUG: Full prompt length: 7814
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16275 characters
DEBUG: Successfully extracted response, length: 3887
DEBUG: call_ollama_api started with prompt length: 15608, system_prompt length: 16
DEBUG: Using model: gemma2:2b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 15628
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 21969 characters
DEBUG: Successfully extracted response, length: 2561
DEBUG: call_ollama_api started with prompt length: 15922, system_prompt length: 15
DEBUG: Using model: gemma2:2b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 15941
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 26105 characters
DEBUG: Successfully extracted response, length: 4558
DEBUG: call_ollama_api started with prompt length: 5728, system_prompt length: 16
DEBUG: Using model: gemma2:2b, temperature: 0.7, max_tokens: 8192
DEBUG: Full prompt length: 5748
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 8925, system_prompt length: 169
DEBUG: Using model: llama3.2:3b, temperature: 0.8, max_tokens: 8192
DEBUG: Full prompt length: 9098
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 17140 characters
DEBUG: Successfully extracted response, length: 4575
DEBUG: call_ollama_api started with prompt length: 5020, system_prompt length: 177
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 8192
DEBUG: Full prompt length: 5201
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 5399 characters
DEBUG: Successfully extracted response, length: 205
DEBUG: call_ollama_api started with prompt length: 5020, system_prompt length: 177
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 8192
DEBUG: Full prompt length: 5201
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 14002 characters
DEBUG: Successfully extracted response, length: 4710
DEBUG: call_ollama_api started with prompt length: 13863, system_prompt length: 169
DEBUG: Using model: llama3.2:3b, temperature: 0.8, max_tokens: 8192
DEBUG: Full prompt length: 14036
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 18247 characters
DEBUG: Successfully extracted response, length: 2783
DEBUG: call_ollama_api started with prompt length: 3228, system_prompt length: 177
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 8192
DEBUG: Full prompt length: 3409
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 10671 characters
DEBUG: Successfully extracted response, length: 3805
DEBUG: call_ollama_api started with prompt length: 3228, system_prompt length: 177
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 8192
DEBUG: Full prompt length: 3409
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 12302 characters
DEBUG: Successfully extracted response, length: 4647
DEBUG: call_ollama_api started with prompt length: 6453, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 8192, max_tokens: 1
DEBUG: Full prompt length: 6473
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 9895 characters
DEBUG: Successfully extracted response, length: 1843
DEBUG: call_ollama_api started with prompt length: 7126, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 8192, max_tokens: 1
DEBUG: Full prompt length: 7146
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15506 characters
DEBUG: Successfully extracted response, length: 4513
DEBUG: call_ollama_api started with prompt length: 15615, system_prompt length: 169
DEBUG: Using model: llama3.2:3b, temperature: 0.8, max_tokens: 8192
DEBUG: Full prompt length: 15788
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 14585 characters
DEBUG: Successfully extracted response, length: 25
DEBUG: call_ollama_api started with prompt length: 468, system_prompt length: 177
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 8192
DEBUG: Full prompt length: 649
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 4390 characters
DEBUG: Successfully extracted response, length: 1782
DEBUG: call_ollama_api started with prompt length: 468, system_prompt length: 177
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 8192
DEBUG: Full prompt length: 649
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 14759 characters
DEBUG: Successfully extracted response, length: 6736
DEBUG: call_ollama_api started with prompt length: 5293, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 8192, max_tokens: 1
DEBUG: Full prompt length: 5313
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 10600 characters
DEBUG: Successfully extracted response, length: 2699
DEBUG: call_ollama_api started with prompt length: 5594, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 8192, max_tokens: 1
DEBUG: Full prompt length: 5614
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 11804 characters
DEBUG: Successfully extracted response, length: 2788
DEBUG: call_ollama_api started with prompt length: 24190, system_prompt length: 169
DEBUG: Using model: llama3.2:3b, temperature: 0.8, max_tokens: 8192
DEBUG: Full prompt length: 24363
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 23063 characters
DEBUG: Successfully extracted response, length: 29
DEBUG: call_ollama_api started with prompt length: 472, system_prompt length: 177
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 8192
DEBUG: Full prompt length: 653
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 1930 characters
DEBUG: Successfully extracted response, length: 475
DEBUG: call_ollama_api started with prompt length: 472, system_prompt length: 177
DEBUG: Using model: llama3.2:1b, temperature: 0.8, max_tokens: 8192
DEBUG: Full prompt length: 653
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 11427 characters
DEBUG: Successfully extracted response, length: 5346
DEBUG: call_ollama_api started with prompt length: 7026, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 8192, max_tokens: 1
DEBUG: Full prompt length: 7046
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 11002 characters
DEBUG: Successfully extracted response, length: 1949
DEBUG: call_ollama_api started with prompt length: 7670, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 8192, max_tokens: 1
DEBUG: Full prompt length: 7689
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 12907 characters
DEBUG: Successfully extracted response, length: 2884
DEBUG: call_ollama_api started with prompt length: 4054, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 8192, max_tokens: 1
DEBUG: Full prompt length: 4074
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 10303 characters
DEBUG: Successfully extracted response, length: 3502
DEBUG: call_ollama_api started with prompt length: 4698, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 8192, max_tokens: 1
DEBUG: Full prompt length: 4717
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 12544 characters
DEBUG: Successfully extracted response, length: 4598
DEBUG: call_ollama_api started with prompt length: 5768, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 8192, max_tokens: 1
DEBUG: Full prompt length: 5788
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 11453 characters
DEBUG: Successfully extracted response, length: 3423
DEBUG: call_ollama_api started with prompt length: 6412, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 8192, max_tokens: 1
DEBUG: Full prompt length: 6431
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 12738 characters
DEBUG: Successfully extracted response, length: 3859
DEBUG: call_ollama_api started with prompt length: 5029, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 8192, max_tokens: 1
DEBUG: Full prompt length: 5049
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 9059 characters
DEBUG: Successfully extracted response, length: 2338
DEBUG: call_ollama_api started with prompt length: 5673, system_prompt length: 15
DEBUG: Using model: llama3.2:1b, temperature: 8192, max_tokens: 1
DEBUG: Full prompt length: 5692
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 24969 characters
DEBUG: Successfully extracted response, length: 9231
DEBUG: call_ollama_api started with prompt length: 10401, system_prompt length: 16
DEBUG: Using model: llama3.2:1b, temperature: 8192, max_tokens: 1
DEBUG: Full prompt length: 10421
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 906, system_prompt length: 725
DEBUG: Using model: gemma2:2b, temperature: 0.7, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1725
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 12124 characters
DEBUG: Successfully extracted response, length: 4493
DEBUG: call_ollama_api started with prompt length: 4837, system_prompt length: 725
DEBUG: Using model: gemma2:2b, temperature: 0.7, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 5656
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16222 characters
DEBUG: Successfully extracted response, length: 4487
DEBUG: call_ollama_api started with prompt length: 4849, system_prompt length: 725
DEBUG: Using model: gemma2:2b, temperature: 0.7, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 5668
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16530 characters
DEBUG: Successfully extracted response, length: 4626
DEBUG: call_ollama_api started with prompt length: 5727, system_prompt length: 169
DEBUG: Using model: gemma2:2b, temperature: 1.0, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 5990
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 14876 characters
DEBUG: Successfully extracted response, length: 3841
DEBUG: call_ollama_api started with prompt length: 4286, system_prompt length: 177
DEBUG: Using model: qwen2.5:1.5b, temperature: 1.0, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 4495
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 17329 characters
DEBUG: Successfully extracted response, length: 6519
DEBUG: call_ollama_api started with prompt length: 4286, system_prompt length: 177
DEBUG: Using model: qwen2.5:1.5b, temperature: 1.0, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 4495
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 22005 characters
DEBUG: Successfully extracted response, length: 8947
DEBUG: call_ollama_api started with prompt length: 973, system_prompt length: 797
DEBUG: Using model: llama3.1:8b, temperature: 0.7, max_tokens: 0.95, ctx: 8192
DEBUG: Using formatted prompt length: 1813
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 1004, system_prompt length: 797
DEBUG: Using model: llama3.1:8b, temperature: 0.7, max_tokens: 0.95, ctx: 8192
DEBUG: Using formatted prompt length: 1844
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 1414, system_prompt length: 797
DEBUG: Using model: gemma2:2b, temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 2305
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 18385 characters
DEBUG: Successfully extracted response, length: 7652
DEBUG: call_ollama_api started with prompt length: 7996, system_prompt length: 797
DEBUG: Using model: gemma2:2b, temperature: 0.5, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 8887
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 24506 characters
DEBUG: Successfully extracted response, length: 7667
DEBUG: call_ollama_api started with prompt length: 8029, system_prompt length: 797
DEBUG: Using model: gemma2:2b, temperature: 0.4, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 8920
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 24550 characters
DEBUG: Successfully extracted response, length: 7668
DEBUG: call_ollama_api started with prompt length: 972, system_prompt length: 797
DEBUG: Using model: gemma2:2b, temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1863
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 9962 characters
DEBUG: Successfully extracted response, length: 3409
DEBUG: call_ollama_api started with prompt length: 54229, system_prompt length: 797
DEBUG: Using model: gemma2:2b, temperature: 0.5, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 55120
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 1324, system_prompt length: 797
DEBUG: Using model: gemma2:2b, temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 2215
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15422 characters
DEBUG: Successfully extracted response, length: 5435
DEBUG: call_ollama_api started with prompt length: 5779, system_prompt length: 797
DEBUG: Using model: gemma2:2b, temperature: 0.5, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 6670
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 917, system_prompt length: 797
DEBUG: Using model: gemma2:2b, temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1808
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 19236 characters
DEBUG: Successfully extracted response, length: 7409
DEBUG: call_ollama_api started with prompt length: 7753, system_prompt length: 797
DEBUG: Using model: gemma2:2b, temperature: 0.5, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 8644
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 27306 characters
DEBUG: Successfully extracted response, length: 7394
DEBUG: call_ollama_api started with prompt length: 7756, system_prompt length: 797
DEBUG: Using model: gemma2:2b, temperature: 0.4, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 8647
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 27320 characters
DEBUG: Successfully extracted response, length: 7378
DEBUG: call_ollama_api started with prompt length: 8596, system_prompt length: 169
DEBUG: Using model: gemma2:2b, temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 8859
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 19914 characters
DEBUG: Successfully extracted response, length: 4391
DEBUG: call_ollama_api started with prompt length: 5264, system_prompt length: 606
DEBUG: Using model: llama3.1:8b, temperature: 0.95, max_tokens: 8192, ctx: 8192
DEBUG: Using formatted prompt length: 5913
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 13873 characters
DEBUG: Successfully extracted response, length: 4350
DEBUG: call_ollama_api started with prompt length: 5264, system_prompt length: 606
DEBUG: Using model: llama3.1:8b, temperature: 0.95, max_tokens: 8192, ctx: 8192
DEBUG: Using formatted prompt length: 5913
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 903, system_prompt length: 797
DEBUG: Using model: gemma2:2b, temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1794
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16180 characters
DEBUG: Successfully extracted response, length: 6346
DEBUG: call_ollama_api started with prompt length: 6690, system_prompt length: 797
DEBUG: Using model: qwen2:7b-instruct-q4_K_M, temperature: 0.5, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 7519
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 19767 characters
DEBUG: Successfully extracted response, length: 6080
DEBUG: call_ollama_api started with prompt length: 6442, system_prompt length: 797
DEBUG: Using model: gemma2:9b-instruct-q4_K_M, temperature: 0.4, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 7333
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 22668 characters
DEBUG: Successfully extracted response, length: 6917
DEBUG: call_ollama_api started with prompt length: 7316, system_prompt length: 169
DEBUG: Using model: llama3.1:8b-instruct-q4_K_M, temperature: 0.95, max_tokens: 64000, ctx: 8192
DEBUG: Using formatted prompt length: 7528
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 17552 characters
DEBUG: Successfully extracted response, length: 5252
DEBUG: call_ollama_api started with prompt length: 6125, system_prompt length: 606
DEBUG: Using model: llama3.1:8b, temperature: 0.95, max_tokens: 8192, ctx: 8192
DEBUG: Using formatted prompt length: 6774
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15497 characters
DEBUG: Successfully extracted response, length: 4815
DEBUG: call_ollama_api started with prompt length: 6125, system_prompt length: 606
DEBUG: Using model: llama3.1:8b, temperature: 0.95, max_tokens: 8192, ctx: 8192
DEBUG: Using formatted prompt length: 6774
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 13327 characters
DEBUG: Successfully extracted response, length: 3796
DEBUG: call_ollama_api started with prompt length: 15039, system_prompt length: 32
DEBUG: Using model: gemma2:2b, temperature: 0.95, max_tokens: 4096, ctx: 4096
DEBUG: Using formatted prompt length: 15165
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 99389 characters
DEBUG: Successfully extracted response, length: 5013
DEBUG: call_ollama_api started with prompt length: 15328, system_prompt length: 32
DEBUG: Using model: granite3.3:2b, temperature: 0.95, max_tokens: 8192, ctx: 16384
DEBUG: Using formatted prompt length: 15392
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 26926 characters
DEBUG: Successfully extracted response, length: 4700
DEBUG: call_ollama_api started with prompt length: 5870, system_prompt length: 32
DEBUG: Using model: gemma2:2b, temperature: 0.95, max_tokens: 4096, ctx: 4096
DEBUG: Using formatted prompt length: 5996
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 52144 characters
DEBUG: Successfully extracted response, length: 2640
DEBUG: call_ollama_api started with prompt length: 21208, system_prompt length: 169
DEBUG: Using model: 1, temperature: 0.95, max_tokens: 64000, ctx: 4096
DEBUG: Using formatted prompt length: 21409
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 31 characters
DEBUG: Failed to extract .response from JSON, error: model '1' not found
DEBUG: call_ollama_api started with prompt length: 21208, system_prompt length: 169
DEBUG: Using model: llama3.1:8b, temperature: 0.95, max_tokens: 64000, ctx: 8192
DEBUG: Using formatted prompt length: 21420
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 21208, system_prompt length: 169
DEBUG: Using model: llama3.1:8b, temperature: 0.95, max_tokens: 64000, ctx: 8192
DEBUG: Using formatted prompt length: 21420
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 21208, system_prompt length: 0
DEBUG: Using model: llama3.1:8b, temperature: 0.7, max_tokens: 8000, ctx: 8192
DEBUG: Using formatted prompt length: 21251
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 21208, system_prompt length: 32
DEBUG: Using model: , temperature: 0.7, max_tokens: 8000, ctx: 4096
DEBUG: Using formatted prompt length: 21272
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 30 characters
DEBUG: Failed to extract .response from JSON, error: model '' not found
DEBUG: call_ollama_api started with prompt length: 21208, system_prompt length: 32
DEBUG: Using model: , temperature: 0.7, max_tokens: 8000, ctx: 4096
DEBUG: Using formatted prompt length: 21272
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 30 characters
DEBUG: Failed to extract .response from JSON, error: model '' not found
DEBUG: call_ollama_api started with prompt length: 21208, system_prompt length: 32
DEBUG: Using model: , temperature: 0.7, max_tokens: 8000, ctx: 4096
DEBUG: Using formatted prompt length: 21272
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 30 characters
DEBUG: Failed to extract .response from JSON, error: model '' not found
DEBUG: call_ollama_api started with prompt length: 21208, system_prompt length: 32
DEBUG: Using model: , temperature: 0.7, max_tokens: 8000, ctx: 4096
DEBUG: Using formatted prompt length: 21272
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 30 characters
DEBUG: Failed to extract .response from JSON, error: model '' not found
DEBUG: call_ollama_api started with prompt length: 21208, system_prompt length: 32
DEBUG: Using model: , temperature: 0.7, max_tokens: 8000, ctx: 4096
DEBUG: Using formatted prompt length: 21272
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 30 characters
DEBUG: Failed to extract .response from JSON, error: model '' not found
DEBUG: call_ollama_api started with prompt length: 21208, system_prompt length: 32
DEBUG: Using model: , temperature: 0.7, max_tokens: 8000, ctx: 4096
DEBUG: Using formatted prompt length: 21272
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 30 characters
DEBUG: Failed to extract .response from JSON, error: model '' not found
DEBUG: call_ollama_api started with prompt length: 21208, system_prompt length: 32
DEBUG: Using model: , temperature: 0.7, max_tokens: 8000, ctx: 4096
DEBUG: Using formatted prompt length: 21272
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 30 characters
DEBUG: Failed to extract .response from JSON, error: model '' not found
DEBUG: call_ollama_api started with prompt length: 21208, system_prompt length: 32
DEBUG: Using model: , temperature: 0.7, max_tokens: 8000, ctx: 4096
DEBUG: Using formatted prompt length: 21272
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 30 characters
DEBUG: Failed to extract .response from JSON, error: model '' not found
DEBUG: call_ollama_api started with prompt length: 21208, system_prompt length: 169
DEBUG: Using model: , temperature: 0.95, max_tokens: 64000, ctx: 4096
DEBUG: Using formatted prompt length: 21409
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 21208, system_prompt length: 169
DEBUG: Using model: , temperature: 0.95, max_tokens: 64000, ctx: 4096
DEBUG: Using formatted prompt length: 21409
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 21208, system_prompt length: 169
DEBUG: Using model: , temperature: 0.95, max_tokens: 64000, ctx: 4096
DEBUG: Using formatted prompt length: 21409
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 27722 characters
DEBUG: Successfully extracted response, length: 4336
DEBUG: call_ollama_api started with prompt length: 26935, system_prompt length: 60
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 27027
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 33346, system_prompt length: 169
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 33547
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 0 characters
DEBUG: Failed to extract .response from JSON, error: 
DEBUG: call_ollama_api started with prompt length: 33346, system_prompt length: 169
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 33547
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 0 characters
DEBUG: Failed to extract .response from JSON, error: 
DEBUG: call_ollama_api started with prompt length: 33346, system_prompt length: 169
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 33547
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 0 characters
DEBUG: Failed to extract .response from JSON, error: 
DEBUG: call_ollama_api started with prompt length: 33346, system_prompt length: 169
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 33547
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 0 characters
DEBUG: Failed to extract .response from JSON, error: 
DEBUG: call_ollama_api started with prompt length: 33346, system_prompt length: 169
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 33547
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 0 characters
DEBUG: Failed to extract .response from JSON, error: 
DEBUG: call_ollama_api started with prompt length: 33346, system_prompt length: 169
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 33547
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 57 characters
DEBUG: Failed to extract .response from JSON, error: model 'qwen2.5:3b-instruct-q4_K_M ' not found
DEBUG: call_ollama_api started with prompt length: 33346, system_prompt length: 169
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 33547
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 39 characters
DEBUG: Failed to extract .response from JSON, error: model 'llama3:8b' not found
DEBUG: call_ollama_api started with prompt length: 33346, system_prompt length: 169
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 33547
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 40227 characters
DEBUG: Successfully extracted response, length: 4920
DEBUG: call_ollama_api started with prompt length: 18093, system_prompt length: 60
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 18185
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 46068, system_prompt length: 169
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 46269
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 58316 characters
DEBUG: Successfully extracted response, length: 8389
DEBUG: call_ollama_api started with prompt length: 24016, system_prompt length: 60
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 24108
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 33963 characters
DEBUG: Successfully extracted response, length: 3200
DEBUG: call_ollama_api started with prompt length: 24305, system_prompt length: 122
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 24459
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 4935, system_prompt length: 169
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 5136
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16587 characters
DEBUG: Successfully extracted response, length: 4186
DEBUG: call_ollama_api started with prompt length: 10200, system_prompt length: 606
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 10838
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 39 characters
DEBUG: Failed to extract .response from JSON, error: model 'phi2:2.7b' not found
DEBUG: call_ollama_api started with prompt length: 10200, system_prompt length: 606
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 10838
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 39 characters
DEBUG: Failed to extract .response from JSON, error: model 'llama3:8b' not found
DEBUG: call_ollama_api started with prompt length: 10200, system_prompt length: 606
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 10838
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16945 characters
DEBUG: Successfully extracted response, length: 3488
DEBUG: call_ollama_api started with prompt length: 11631, system_prompt length: 60
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 11723
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 21871 characters
DEBUG: Successfully extracted response, length: 4190
DEBUG: call_ollama_api started with prompt length: 11920, system_prompt length: 122
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 12074
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 22544 characters
DEBUG: Successfully extracted response, length: 4172
DEBUG: call_ollama_api started with prompt length: 17428, system_prompt length: 60
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 17520
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 27546 characters
DEBUG: Successfully extracted response, length: 4020
DEBUG: call_ollama_api started with prompt length: 17717, system_prompt length: 122
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 17871
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 10549, system_prompt length: 169
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 10750
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 33864 characters
DEBUG: Successfully extracted response, length: 10126
DEBUG: call_ollama_api started with prompt length: 9056, system_prompt length: 60
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 9148
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 17745 characters
DEBUG: Successfully extracted response, length: 3067
DEBUG: call_ollama_api started with prompt length: 9345, system_prompt length: 122
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 9499
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 31450, system_prompt length: 169
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 31651
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 49198 characters
DEBUG: Successfully extracted response, length: 5985
DEBUG: call_ollama_api started with prompt length: 16541, system_prompt length: 122
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 16695
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 7748, system_prompt length: 169
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 7949
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 24526 characters
DEBUG: Successfully extracted response, length: 7800
DEBUG: call_ollama_api started with prompt length: 17220, system_prompt length: 122
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 17374
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 30842 characters
DEBUG: Successfully extracted response, length: 5398
DEBUG: call_ollama_api started with prompt length: 17220, system_prompt length: 122
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 17374
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 899, system_prompt length: 797
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1728
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 911, system_prompt length: 797
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1740
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 920, system_prompt length: 797
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1749
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 0 characters
DEBUG: Failed to extract .response from JSON, error: 
DEBUG: call_ollama_api started with prompt length: 920, system_prompt length: 797
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1749
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 39 characters
DEBUG: Failed to extract .response from JSON, error: model 'llama3:8b' not found
DEBUG: call_ollama_api started with prompt length: 920, system_prompt length: 797
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1749
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 0 characters
DEBUG: Failed to extract .response from JSON, error: 
DEBUG: call_ollama_api started with prompt length: 920, system_prompt length: 797
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1749
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 43 characters
DEBUG: Failed to extract .response from JSON, error: model 'llama3:latest' not found
DEBUG: call_ollama_api started with prompt length: 920, system_prompt length: 797
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1749
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 38 characters
DEBUG: Failed to extract .response from JSON, error: model 'gemma:7b' not found
DEBUG: call_ollama_api started with prompt length: 920, system_prompt length: 797
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1749
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 0 characters
DEBUG: Failed to extract .response from JSON, error: 
DEBUG: call_ollama_api started with prompt length: 920, system_prompt length: 797
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1749
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 44 characters
DEBUG: Failed to extract .response from JSON, error: model 'mixtral:latest' not found
DEBUG: call_ollama_api started with prompt length: 920, system_prompt length: 797
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1749
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 43 characters
DEBUG: Failed to extract .response from JSON, error: model 'llama2:latest' not found
DEBUG: call_ollama_api started with prompt length: 895, system_prompt length: 797
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1724
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 24577 characters
DEBUG: Successfully extracted response, length: 10005
DEBUG: call_ollama_api started with prompt length: 10349, system_prompt length: 797
DEBUG: Using model: , temperature: 0.5, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 11178
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 855, system_prompt length: 797
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1684
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 103770 characters
DEBUG: Successfully extracted response, length: 39132
DEBUG: call_ollama_api started with prompt length: 39476, system_prompt length: 797
DEBUG: Using model: , temperature: 0.5, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 40305
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 65112 characters
DEBUG: Successfully extracted response, length: 1964
DEBUG: call_ollama_api started with prompt length: 2326, system_prompt length: 797
DEBUG: Using model: , temperature: 0.4, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 3155
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 944, system_prompt length: 797
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1773
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 12028 characters
DEBUG: Successfully extracted response, length: 5002
DEBUG: call_ollama_api started with prompt length: 5346, system_prompt length: 797
DEBUG: Using model: , temperature: 0.5, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 6175
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 34226 characters
DEBUG: Successfully extracted response, length: 15127
DEBUG: call_ollama_api started with prompt length: 15489, system_prompt length: 797
DEBUG: Using model: , temperature: 0.4, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 16318
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 30418 characters
DEBUG: Successfully extracted response, length: 8789
DEBUG: call_ollama_api started with prompt length: 1150, system_prompt length: 797
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1979
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 8781 characters
DEBUG: Successfully extracted response, length: 3086
DEBUG: call_ollama_api started with prompt length: 3430, system_prompt length: 797
DEBUG: Using model: , temperature: 0.5, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 4259
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16416 characters
DEBUG: Successfully extracted response, length: 6116
DEBUG: call_ollama_api started with prompt length: 6478, system_prompt length: 797
DEBUG: Using model: , temperature: 0.4, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 7307
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 18721 characters
DEBUG: Successfully extracted response, length: 6116
DEBUG: call_ollama_api started with prompt length: 888, system_prompt length: 707
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1627
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 14353 characters
DEBUG: Successfully extracted response, length: 6274
DEBUG: call_ollama_api started with prompt length: 6618, system_prompt length: 707
DEBUG: Using model: , temperature: 0.5, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 7357
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 825, system_prompt length: 707
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1564
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 870, system_prompt length: 707
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1609
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 8128 characters
DEBUG: Successfully extracted response, length: 3015
DEBUG: call_ollama_api started with prompt length: 843, system_prompt length: 707
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1582
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15190 characters
DEBUG: Successfully extracted response, length: 6985
DEBUG: call_ollama_api started with prompt length: 8916, system_prompt length: 707
DEBUG: Using model: , temperature: 0.5, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 9655
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 855, system_prompt length: 707
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1594
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 7148 characters
DEBUG: Successfully extracted response, length: 2470
DEBUG: call_ollama_api started with prompt length: 4413, system_prompt length: 707
DEBUG: Using model: , temperature: 0.5, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 5152
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 914, system_prompt length: 707
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1653
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 19720 characters
DEBUG: Successfully extracted response, length: 8889
DEBUG: call_ollama_api started with prompt length: 10891, system_prompt length: 707
DEBUG: Using model: , temperature: 0.5, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 11630
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 873, system_prompt length: 707
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1612
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15256 characters
DEBUG: Successfully extracted response, length: 6649
DEBUG: call_ollama_api started with prompt length: 8610, system_prompt length: 707
DEBUG: Using model: , temperature: 0.5, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 9349
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 22761 characters
DEBUG: Successfully extracted response, length: 6649
DEBUG: call_ollama_api started with prompt length: 16364, system_prompt length: 707
DEBUG: Using model: , temperature: 0.4, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 17103
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 871, system_prompt length: 707
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1610
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 9533 characters
DEBUG: Successfully extracted response, length: 3822
DEBUG: call_ollama_api started with prompt length: 4166, system_prompt length: 707
DEBUG: Using model: , temperature: 0.5, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 4905
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16874 characters
DEBUG: Successfully extracted response, length: 5985
DEBUG: call_ollama_api started with prompt length: 6347, system_prompt length: 707
DEBUG: Using model: , temperature: 0.4, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 7086
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 934, system_prompt length: 707
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1673
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15304 characters
DEBUG: Successfully extracted response, length: 6331
DEBUG: call_ollama_api started with prompt length: 6744, system_prompt length: 707
DEBUG: Using model: , temperature: 0.5, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 7483
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 27224 characters
DEBUG: Successfully extracted response, length: 9552
DEBUG: call_ollama_api started with prompt length: 9983, system_prompt length: 707
DEBUG: Using model: , temperature: 0.4, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 10722
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 29554 characters
DEBUG: Successfully extracted response, length: 8214
DEBUG: call_ollama_api started with prompt length: 859, system_prompt length: 707
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1598
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15475 characters
DEBUG: Successfully extracted response, length: 6738
DEBUG: call_ollama_api started with prompt length: 7151, system_prompt length: 707
DEBUG: Using model: , temperature: 0.5, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 7890
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 20023 characters
DEBUG: Successfully extracted response, length: 5988
DEBUG: call_ollama_api started with prompt length: 957, system_prompt length: 707
DEBUG: Using model: , temperature: 0.4, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1696
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 8127 characters
DEBUG: Successfully extracted response, length: 3065
DEBUG: call_ollama_api started with prompt length: 10915, system_prompt length: 169
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 11116
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 17460 characters
DEBUG: Successfully extracted response, length: 3335
DEBUG: call_ollama_api started with prompt length: 871, system_prompt length: 699
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1602
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 10915, system_prompt length: 169
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 11116
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 41890 characters
DEBUG: Successfully extracted response, length: 16182
DEBUG: call_ollama_api started with prompt length: 11495, system_prompt length: 699
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 12226
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 861, system_prompt length: 707
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 1600
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 9483 characters
DEBUG: Successfully extracted response, length: 3790
DEBUG: call_ollama_api started with prompt length: 4203, system_prompt length: 707
DEBUG: Using model: , temperature: 0.5, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 4942
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 14266 characters
DEBUG: Successfully extracted response, length: 4764
DEBUG: call_ollama_api started with prompt length: 5195, system_prompt length: 707
DEBUG: Using model: , temperature: 0.4, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 5934
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15876 characters
DEBUG: Successfully extracted response, length: 4675
DEBUG: call_ollama_api started with prompt length: 791, system_prompt length: 656
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 1479
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16505 characters
DEBUG: Successfully extracted response, length: 7822
DEBUG: call_ollama_api started with prompt length: 8179, system_prompt length: 195
DEBUG: Using model: , temperature: 0.5, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 8406
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 850, system_prompt length: 656
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 1538
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15947 characters
DEBUG: Successfully extracted response, length: 6559
DEBUG: call_ollama_api started with prompt length: 6916, system_prompt length: 195
DEBUG: Using model: , temperature: 0.5, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 7143
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 23048 characters
DEBUG: Successfully extracted response, length: 7003
DEBUG: call_ollama_api started with prompt length: 7353, system_prompt length: 197
DEBUG: Using model: , temperature: 0.4, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 7582
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 22636 characters
DEBUG: Successfully extracted response, length: 6751
DEBUG: call_ollama_api started with prompt length: 0, system_prompt length: 0
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 32
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 538 characters
DEBUG: Successfully extracted response, length: 0
DEBUG: call_ollama_api started with prompt length: 662, system_prompt length: 656
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 1350
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 789, system_prompt length: 656
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 1477
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 23485 characters
DEBUG: Successfully extracted response, length: 11225
DEBUG: call_ollama_api started with prompt length: 11582, system_prompt length: 195
DEBUG: Using model: , temperature: 0.5, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 11809
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 662, system_prompt length: 656
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 1350
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 8384 characters
DEBUG: Successfully extracted response, length: 3351
DEBUG: call_ollama_api started with prompt length: 753, system_prompt length: 656
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 1441
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 10971 characters
DEBUG: Successfully extracted response, length: 4382
DEBUG: call_ollama_api started with prompt length: 5627, system_prompt length: 368
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 6027
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15151 characters
DEBUG: Successfully extracted response, length: 4507
DEBUG: call_ollama_api started with prompt length: 5012, system_prompt length: 368
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 5412
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 20508 characters
DEBUG: Successfully extracted response, length: 8028
DEBUG: call_ollama_api started with prompt length: 13678, system_prompt length: 368
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 14078
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 23571 characters
DEBUG: Successfully extracted response, length: 5203
DEBUG: call_ollama_api started with prompt length: 5628, system_prompt length: 368
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 6028
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 25814 characters
DEBUG: Successfully extracted response, length: 10722
DEBUG: call_ollama_api started with prompt length: 10978, system_prompt length: 368
DEBUG: Using model: , temperature: 0.6, max_tokens: 2048, ctx: 4096
DEBUG: Using formatted prompt length: 11378
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 815, system_prompt length: 656
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 1503
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 12909 characters
DEBUG: Successfully extracted response, length: 5669
DEBUG: call_ollama_api started with prompt length: 1323, system_prompt length: 1188
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 2543
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 17023 characters
DEBUG: Successfully extracted response, length: 7318
DEBUG: call_ollama_api started with prompt length: 1423, system_prompt length: 1188
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 2643
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16504 characters
DEBUG: Successfully extracted response, length: 6861
DEBUG: call_ollama_api started with prompt length: 8580, system_prompt length: 368
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 8980
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15175 characters
DEBUG: Successfully extracted response, length: 3528
DEBUG: call_ollama_api started with prompt length: 4033, system_prompt length: 368
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 4433
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 13832 characters
DEBUG: Successfully extracted response, length: 4286
DEBUG: call_ollama_api started with prompt length: 4541, system_prompt length: 368
DEBUG: Using model: , temperature: 0.6, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 4941
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 13373 characters
DEBUG: Successfully extracted response, length: 4657
DEBUG: call_ollama_api started with prompt length: 9202, system_prompt length: 368
DEBUG: Using model: , temperature: 0.6, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 9602
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 22903 characters
DEBUG: Successfully extracted response, length: 7527
DEBUG: call_ollama_api started with prompt length: 25092, system_prompt length: 368
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 25492
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 34184 characters
DEBUG: Successfully extracted response, length: 5825
DEBUG: call_ollama_api started with prompt length: 4272, system_prompt length: 368
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 4672
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 19066 characters
DEBUG: Successfully extracted response, length: 6530
DEBUG: call_ollama_api started with prompt length: 3261, system_prompt length: 368
DEBUG: Using model: , temperature: 0.6, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 3661
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 12768 characters
DEBUG: Successfully extracted response, length: 4794
DEBUG: call_ollama_api started with prompt length: 8059, system_prompt length: 368
DEBUG: Using model: , temperature: 0.6, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 8459
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 18389 characters
DEBUG: Successfully extracted response, length: 5528
DEBUG: call_ollama_api started with prompt length: 13590, system_prompt length: 368
DEBUG: Using model: , temperature: 0.6, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 13990
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 69828 characters
DEBUG: Successfully extracted response, length: 29767
DEBUG: call_ollama_api started with prompt length: 25159, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 25637
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 29247 characters
DEBUG: Successfully extracted response, length: 3202
DEBUG: call_ollama_api started with prompt length: 3864, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 2076, ctx: 4096
DEBUG: Using formatted prompt length: 4342
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 14407 characters
DEBUG: Successfully extracted response, length: 4362
DEBUG: call_ollama_api started with prompt length: 4868, system_prompt length: 446
DEBUG: Using model: , temperature: 0.6, max_tokens: 1912, ctx: 4096
DEBUG: Using formatted prompt length: 5346
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15131 characters
DEBUG: Successfully extracted response, length: 5239
DEBUG: call_ollama_api started with prompt length: 6709, system_prompt length: 446
DEBUG: Using model: , temperature: 0.6, max_tokens: 869, ctx: 4096
DEBUG: Using formatted prompt length: 7187
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16683 characters
DEBUG: Successfully extracted response, length: 5214
DEBUG: call_ollama_api started with prompt length: 23517, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 128000, ctx: 4096
DEBUG: Using formatted prompt length: 23995
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 16452, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: , ctx: 4096
DEBUG: Using formatted prompt length: 16930
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 32 characters
DEBUG: Failed to extract .response from JSON, error: missing request body
DEBUG: call_ollama_api started with prompt length: 16452, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: , ctx: 4096
DEBUG: Using formatted prompt length: 16930
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 32 characters
DEBUG: Failed to extract .response from JSON, error: missing request body
DEBUG: call_ollama_api started with prompt length: 16452, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: , ctx: 4096
DEBUG: Using formatted prompt length: 16930
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 32 characters
DEBUG: Failed to extract .response from JSON, error: missing request body
DEBUG: call_ollama_api started with prompt length: 16452, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: , ctx: 4096
DEBUG: Using formatted prompt length: 16930
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 32 characters
DEBUG: Failed to extract .response from JSON, error: missing request body
DEBUG: call_ollama_api started with prompt length: 16452, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: , ctx: 4096
DEBUG: Using formatted prompt length: 16930
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 32 characters
DEBUG: Failed to extract .response from JSON, error: missing request body
DEBUG: call_ollama_api started with prompt length: 16452, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: , ctx: 4096
DEBUG: Using formatted prompt length: 16930
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 32 characters
DEBUG: Failed to extract .response from JSON, error: missing request body
DEBUG: call_ollama_api started with prompt length: 16452, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: , ctx: 4096
DEBUG: Using formatted prompt length: 16930
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 32 characters
DEBUG: Failed to extract .response from JSON, error: missing request body
DEBUG: call_ollama_api started with prompt length: 16452, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 16930
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 58406 characters
DEBUG: Successfully extracted response, length: 22525
DEBUG: call_ollama_api started with prompt length: 22963, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 23441
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 26077 characters
DEBUG: Successfully extracted response, length: 687
DEBUG: call_ollama_api started with prompt length: 942, system_prompt length: 446
DEBUG: Using model: , temperature: 0.6, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 1420
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 10387 characters
DEBUG: Successfully extracted response, length: 4513
DEBUG: call_ollama_api started with prompt length: 5458, system_prompt length: 446
DEBUG: Using model: , temperature: 0.6, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 5936
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 17889 characters
DEBUG: Successfully extracted response, length: 6187
DEBUG: call_ollama_api started with prompt length: 11648, system_prompt length: 446
DEBUG: Using model: , temperature: 0.6, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 12126
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 8223, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 8701
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15741 characters
DEBUG: Successfully extracted response, length: 3762
DEBUG: call_ollama_api started with prompt length: 4211, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 4689
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15136 characters
DEBUG: Successfully extracted response, length: 4475
DEBUG: call_ollama_api started with prompt length: 4744, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 2319, ctx: 4096
DEBUG: Using formatted prompt length: 4874
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 27326 characters
DEBUG: Successfully extracted response, length: 9814
DEBUG: call_ollama_api started with prompt length: 8403, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 2023, ctx: 4096
DEBUG: Using formatted prompt length: 8533
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 24901 characters
DEBUG: Successfully extracted response, length: 6358
DEBUG: call_ollama_api started with prompt length: 0, system_prompt length: 446
DEBUG: Using model: , temperature: 0.8, max_tokens: 0, ctx: 4096
DEBUG: Using formatted prompt length: 478
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 13892, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 14370
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 33892 characters
DEBUG: Successfully extracted response, length: 10717
DEBUG: call_ollama_api started with prompt length: 10906, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 11384
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 23342 characters
DEBUG: Successfully extracted response, length: 4972
DEBUG: call_ollama_api started with prompt length: 5409, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 970, ctx: 4096
DEBUG: Using formatted prompt length: 5539
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 13218, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 13696
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 27817 characters
DEBUG: Successfully extracted response, length: 7981
DEBUG: call_ollama_api started with prompt length: 8419, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 8897
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15583 characters
DEBUG: Successfully extracted response, length: 4012
DEBUG: call_ollama_api started with prompt length: 3688, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 1236, ctx: 4096
DEBUG: Using formatted prompt length: 3818
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 18084 characters
DEBUG: Successfully extracted response, length: 6697
DEBUG: call_ollama_api started with prompt length: 3861, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 1490, ctx: 4096
DEBUG: Using formatted prompt length: 3991
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 11670, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 12148
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 22574 characters
DEBUG: Successfully extracted response, length: 6077
DEBUG: call_ollama_api started with prompt length: 5909, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 861, ctx: 4096
DEBUG: Using formatted prompt length: 6039
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 23980 characters
DEBUG: Successfully extracted response, length: 8477
DEBUG: call_ollama_api started with prompt length: 6678, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 872, ctx: 4096
DEBUG: Using formatted prompt length: 6808
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 23504, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 23982
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 31747 characters
DEBUG: Successfully extracted response, length: 5225
DEBUG: call_ollama_api started with prompt length: 5356, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 911, ctx: 4096
DEBUG: Using formatted prompt length: 5486
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 21155 characters
DEBUG: Successfully extracted response, length: 7143
DEBUG: call_ollama_api started with prompt length: 15765, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 16243
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 23411 characters
DEBUG: Successfully extracted response, length: 4710
DEBUG: call_ollama_api started with prompt length: 4846, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 1041, ctx: 4096
DEBUG: Using formatted prompt length: 4976
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16680 characters
DEBUG: Successfully extracted response, length: 6576
DEBUG: call_ollama_api started with prompt length: 6107, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 976, ctx: 4096
DEBUG: Using formatted prompt length: 6237
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 19605 characters
DEBUG: Successfully extracted response, length: 6789
DEBUG: call_ollama_api started with prompt length: 0, system_prompt length: 446
DEBUG: Using model: , temperature: 0.8, max_tokens: 0, ctx: 4096
DEBUG: Using formatted prompt length: 478
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 15752, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 16230
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 32002 characters
DEBUG: Successfully extracted response, length: 9373
DEBUG: call_ollama_api started with prompt length: 9853, system_prompt length: 83
DEBUG: Using model: , temperature: 0.7, max_tokens: 3000, ctx: 4096
DEBUG: Using formatted prompt length: 9968
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 32596 characters
DEBUG: Successfully extracted response, length: 13055
DEBUG: call_ollama_api started with prompt length: 21751, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 22229
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 21738, system_prompt length: 446
DEBUG: Using model: , temperature: 0.9, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 22216
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 30840 characters
DEBUG: Successfully extracted response, length: 6031
DEBUG: call_ollama_api started with prompt length: 6467, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 667, ctx: 4096
DEBUG: Using formatted prompt length: 6597
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 17668 characters
DEBUG: Successfully extracted response, length: 6089
DEBUG: call_ollama_api started with prompt length: 6525, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 786, ctx: 4096
DEBUG: Using formatted prompt length: 6655
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 20058 characters
DEBUG: Successfully extracted response, length: 6478
DEBUG: call_ollama_api started with prompt length: 6914, system_prompt length: 446
DEBUG: Using model: , temperature: 0.8, max_tokens: 688, ctx: 4096
DEBUG: Using formatted prompt length: 7392
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 15161, system_prompt length: 446
DEBUG: Using model: , temperature: 0.9, max_tokens: 6500, ctx: 4096
DEBUG: Using formatted prompt length: 15639
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 21519 characters
DEBUG: Successfully extracted response, length: 3708
DEBUG: call_ollama_api started with prompt length: 3077, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 1321, ctx: 4096
DEBUG: Using formatted prompt length: 3207
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 11323, system_prompt length: 446
DEBUG: Using model: , temperature: 0.9, max_tokens: 16000, ctx: 4096
DEBUG: Using formatted prompt length: 11801
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 20124 characters
DEBUG: Successfully extracted response, length: 4656
DEBUG: call_ollama_api started with prompt length: 3678, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 1169, ctx: 4096
DEBUG: Using formatted prompt length: 3808
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 3929 characters
DEBUG: Successfully extracted response, length: 29
DEBUG: call_ollama_api started with prompt length: 464, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 2167, ctx: 4096
DEBUG: Using formatted prompt length: 594
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 2723 characters
DEBUG: Successfully extracted response, length: 773
DEBUG: call_ollama_api started with prompt length: 873, system_prompt length: 446
DEBUG: Using model: , temperature: 0.8, max_tokens: 2059, ctx: 4096
DEBUG: Using formatted prompt length: 1351
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 9120, system_prompt length: 446
DEBUG: Using model: , temperature: 0.9, max_tokens: 64000, ctx: 4096
DEBUG: Using formatted prompt length: 9598
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 20924 characters
DEBUG: Successfully extracted response, length: 6087
DEBUG: call_ollama_api started with prompt length: 5085, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 934, ctx: 4096
DEBUG: Using formatted prompt length: 5215
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 13331, system_prompt length: 446
DEBUG: Using model: , temperature: 0.9, max_tokens: 64000, ctx: 4096
DEBUG: Using formatted prompt length: 13809
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 23770 characters
DEBUG: Successfully extracted response, length: 5625
DEBUG: call_ollama_api started with prompt length: 4747, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 1024, ctx: 4096
DEBUG: Using formatted prompt length: 4877
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16093 characters
DEBUG: Successfully extracted response, length: 5908
DEBUG: call_ollama_api started with prompt length: 4882, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 1202, ctx: 4096
DEBUG: Using formatted prompt length: 5012
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 21861 characters
DEBUG: Successfully extracted response, length: 7739
DEBUG: call_ollama_api started with prompt length: 5473, system_prompt length: 446
DEBUG: Using model: , temperature: 0.8, max_tokens: 1066, ctx: 4096
DEBUG: Using formatted prompt length: 5951
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 13718, system_prompt length: 446
DEBUG: Using model: , temperature: 0.9, max_tokens: 64000, ctx: 4096
DEBUG: Using formatted prompt length: 14196
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 22998 characters
DEBUG: Successfully extracted response, length: 5159
DEBUG: call_ollama_api started with prompt length: 4063, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 1179, ctx: 4096
DEBUG: Using formatted prompt length: 4193
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 13509 characters
DEBUG: Successfully extracted response, length: 4948
DEBUG: call_ollama_api started with prompt length: 4005, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 1423, ctx: 4096
DEBUG: Using formatted prompt length: 4135
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 12249, system_prompt length: 446
DEBUG: Using model: , temperature: 0.9, max_tokens: 64000, ctx: 4096
DEBUG: Using formatted prompt length: 12727
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 20042 characters
DEBUG: Successfully extracted response, length: 4369
DEBUG: call_ollama_api started with prompt length: 4082, system_prompt length: 83
DEBUG: Using model: , temperature: 0.7, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 4197
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 10675 characters
DEBUG: Successfully extracted response, length: 3559
DEBUG: call_ollama_api started with prompt length: 12253, system_prompt length: 446
DEBUG: Using model: , temperature: 0.9, max_tokens: 64000, ctx: 4096
DEBUG: Using formatted prompt length: 12731
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 18681 characters
DEBUG: Successfully extracted response, length: 3637
DEBUG: call_ollama_api started with prompt length: 2423, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 830, ctx: 4096
DEBUG: Using formatted prompt length: 2553
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 9547 characters
DEBUG: Successfully extracted response, length: 3646
DEBUG: call_ollama_api started with prompt length: 2494, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 980, ctx: 4096
DEBUG: Using formatted prompt length: 2624
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 15940 characters
DEBUG: Successfully extracted response, length: 6447
DEBUG: call_ollama_api started with prompt length: 2914, system_prompt length: 446
DEBUG: Using model: , temperature: 0.8, max_tokens: 880, ctx: 4096
DEBUG: Using formatted prompt length: 3392
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 12240, system_prompt length: 446
DEBUG: Using model: , temperature: 0.9, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 12718
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 21006 characters
DEBUG: Successfully extracted response, length: 4935
DEBUG: call_ollama_api started with prompt length: 5065, system_prompt length: 83
DEBUG: Using model: , temperature: 0.7, max_tokens: 3000, ctx: 4096
DEBUG: Using formatted prompt length: 5180
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 14182 characters
DEBUG: Successfully extracted response, length: 4868
DEBUG: call_ollama_api started with prompt length: 13138, system_prompt length: 446
DEBUG: Using model: , temperature: 0.9, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 13616
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 13125, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 13603
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 25737 characters
DEBUG: Successfully extracted response, length: 6912
DEBUG: call_ollama_api started with prompt length: 15032, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 15510
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 22729 characters
DEBUG: Successfully extracted response, length: 4546
DEBUG: call_ollama_api started with prompt length: 19245, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 19723
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 0, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 478
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 8366 characters
DEBUG: Successfully extracted response, length: 0
DEBUG: call_ollama_api started with prompt length: 433, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 1187, ctx: 4096
DEBUG: Using formatted prompt length: 563
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 1181 characters
DEBUG: Successfully extracted response, length: 120
DEBUG: call_ollama_api started with prompt length: 555, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 1390, ctx: 4096
DEBUG: Using formatted prompt length: 685
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 2791 characters
DEBUG: Successfully extracted response, length: 704
DEBUG: call_ollama_api started with prompt length: 801, system_prompt length: 446
DEBUG: Using model: , temperature: 0.8, max_tokens: 1328, ctx: 4096
DEBUG: Using formatted prompt length: 1279
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 15570, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 16048
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 32706 characters
DEBUG: Successfully extracted response, length: 9398
DEBUG: call_ollama_api started with prompt length: 18253, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 18731
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 22613 characters
DEBUG: Successfully extracted response, length: 2850
DEBUG: call_ollama_api started with prompt length: 2040, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 875, ctx: 4096
DEBUG: Using formatted prompt length: 2170
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 11223 characters
DEBUG: Successfully extracted response, length: 4641
DEBUG: call_ollama_api started with prompt length: 3560, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 702, ctx: 4096
DEBUG: Using formatted prompt length: 3690
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 16888 characters
DEBUG: Successfully extracted response, length: 6100
DEBUG: call_ollama_api started with prompt length: 22204, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 8192, ctx: 4096
DEBUG: Using formatted prompt length: 22682
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 28174 characters
DEBUG: Successfully extracted response, length: 3900
DEBUG: call_ollama_api started with prompt length: 3215, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 682, ctx: 4096
DEBUG: Using formatted prompt length: 3345
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 18624, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 19102
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 30834 characters
DEBUG: Successfully extracted response, length: 6870
DEBUG: call_ollama_api started with prompt length: 14063, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 14541
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 25894 characters
DEBUG: Successfully extracted response, length: 6523
DEBUG: call_ollama_api started with prompt length: 18298, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 18776
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 23464 characters
DEBUG: Successfully extracted response, length: 3469
DEBUG: call_ollama_api started with prompt length: 3335, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 704, ctx: 4096
DEBUG: Using formatted prompt length: 3465
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 12030 characters
DEBUG: Successfully extracted response, length: 4735
DEBUG: call_ollama_api started with prompt length: 22298, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 22776
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 27893 characters
DEBUG: Successfully extracted response, length: 4167
DEBUG: call_ollama_api started with prompt length: 26262, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 26740
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 32463 characters
DEBUG: Successfully extracted response, length: 4859
DEBUG: call_ollama_api started with prompt length: 29963, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 30441
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 32952 characters
DEBUG: Successfully extracted response, length: 3481
DEBUG: call_ollama_api started with prompt length: 3236, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 712, ctx: 4096
DEBUG: Using formatted prompt length: 3366
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 11033 characters
DEBUG: Successfully extracted response, length: 4147
DEBUG: call_ollama_api started with prompt length: 3474, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 798, ctx: 4096
DEBUG: Using formatted prompt length: 3604
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 14634 characters
DEBUG: Successfully extracted response, length: 5563
DEBUG: call_ollama_api started with prompt length: 3009, system_prompt length: 446
DEBUG: Using model: , temperature: 0.8, max_tokens: 890, ctx: 4096
DEBUG: Using formatted prompt length: 3487
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 17275, system_prompt length: 446
DEBUG: Using model: , temperature: 0.95, max_tokens: 32000, ctx: 4096
DEBUG: Using formatted prompt length: 17753
DEBUG: About to make curl request to Ollama
DEBUG: call_ollama_api started with prompt length: 9628, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 947, ctx: 4096
DEBUG: Using formatted prompt length: 9758
DEBUG: About to make curl request to Ollama
DEBUG: curl completed with exit code: 0
DEBUG: Response length: 39 characters
DEBUG: Failed to extract .response from JSON, error: model 'llama3:8b' not found
DEBUG: call_ollama_api started with prompt length: 9628, system_prompt length: 98
DEBUG: Using model: , temperature: 0.7, max_tokens: 947, ctx: 4096
DEBUG: Using formatted prompt length: 9758
DEBUG: About to make curl request to Ollama
